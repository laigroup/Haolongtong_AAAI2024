\section{Related work}
\label{sec:Related}
Our work is based on the close relationship between QIF, model counting, and knowledge compilation.
We introduce relevant work from three perspectives: (1) quantitative information flow analysis, (2) model counting, and (3) knowledge compilation.

\textbf{Quantified information flow analysis} 
Currently, the QIF method based on model counting faces two major challenges. 
The first one is to construct the logical postcondition $\Pi_{proc}$ for a program $proc$~\cite{zhou2018static}.
This process can be achieved through symbolic execution, but the existing symbolic execution tools have certain limitations and are difficult to be extended to some complex programs, such as those involving symbolic pointers.
The second challenge is model counting.
Our work focuses on the challenges of model counting. 
For programs modeled by Boolean clause constraints, Shannon entropy can be solved through model counting queries to quantify the amount of information leakage.
Golia et al. have made outstanding contributions to this work. They proposed the first efficient Shannon entropy estimation\cite{golia2022scalable} with PAC guarantees based on sampling and model counting.
Their core idea is to reduce the number of model counting queries through sampling. However, their idea can only provide an approximate estimation of entropy.
Our work is inspired by the research conducted by Golia et al., but our approach and optimization direction differ. 
We optimize the existing framework of model counting for precise Shannon entropy to reduce the number of model counting queries, while simultaneously enhancing the efficiency of model counting solutions. 
Based on this idea, we propose an efficient and precise method for computing the Shannon entropy.
Besides Boolean clause constraints, converting programs into strings and SMT constraints has also attracted extensive research.
For string constraints, Aydin et al.~\cite{aydin2018parameterized} proposed an automata-based model counting solver, MT-ABC, which can be applied to QIF.
Similar solvers also include SMC~\cite{luu2014model}, ABC~\cite{aydin2015automata}, etc. 
The difference between their methods lies in the improvement of the automaton model counting.
Phan et al.~\cite{phan2014abstract} proposed an abstract model counting method, a quantified information leakage method based on an SMT-based framework.
For approximate estimation of leakage, there are many sampling-based methods. 
The most outstanding work currently is the first efficient Shannon entropy estimation with PAC guarantees proposed by Golia et al.~\cite{golia2022scalable}, which is based on sampling and model counting.

\textbf{Model counting}
Since the computation of entropy relies on model counting, we conducted a survey on powerful techniques for model counting.
%In the model counting problem, the algorithm can be divided into exact model counting and approximate model counting based on the accuracy of the solution.
%In exact model counting, the initial algorithm was based on DPLL search, and researchers gradually proposed some optimization strategies, such as cache and component decomposition.
%Since the 21st century, scalable model counters have been mainly divided into three methods~\cite{lai2021power}: (1) search-based, (2) knowledge compilation-based, and (3) variable elimination-based methods. Among these, model counters based on knowledge compilation have received considerable attention in recent years.
Among the existing techniques for exact model counting, the most effective ones mainly include component decomposition, caching, Implicit Binary Constraint Propagation (i-BCP), variable decision heuristics, preprocess, and so on.
The key idea of the disjoint component analysis is to divide the constraint graph into several parts without variable intersection.
However, this idea is not applicable to the computation of Shannon entropy, as the variables in $Y$ do not support component decomposition.
Caching technique has been used in various fields for a long time, and it also has excellent effects in model counting.
Bacchus et al. analysis three distinct caching schemes: simple caching, component caching,and linear-space caching \cite{bacchus2003dpll}. 
Their research showed that component caching has the most strong prospect. 
Combining component caching with conflict driven clause learning is an idea pioneered by Sang et al \cite{sang2004combining} in 2004 with their model counter Cachet which is based on the well-known SAT solver zCHaff \cite{moskewicz2001chaff}. 
We also utilized caching technique in the process of computing entropy, and our experiments once again demonstrated the power of caching technique.
Thurley \cite{thurley2006sharpsat} proposed improved component encoding schemes with a solver called sharpSAT to make cache components more concise. 
sharpSAT first employed i-BCP in model counting, which is an improvement of the well-known "look-ahead" technique based on Boolean constraint propagation. 
The computation of implied literals relies on IBCP, and our experiments have shown that implied literals are also beneficial to the process of computing entropy.
Sharma et al.~\cite{sharma2019ganak} proposed a probabilistic caching strategy in their model counter Ganak, demonstrating its efficiency.
Our tool PSE does not currently integrate a probabilistic caching method, which is a direction for future exploration. 
There have been many studies on variable decision heuristics for model counting, which can be divided into static heuristics and dynamic heuristics. 
In static heuristics, the \textbf{minfill}~\cite{darwiche2009modeling} heuristic is very effective, while in dynamic heuristics, \textbf{VSADS}~\cite{sang2005heuristics}, \textbf{DLCP}~\cite{lai2021power}, and \textbf{SharpSAT-TD  heuristic}~\cite{korhonen2021integrating} have been the most significant in recent years. 
We have already discussed the impact of these heuristics on entropy computation in Section \ref{sec:Experiments}.
Preprocessing methods in model counting have been discussed in detail by Lagniez et al~\cite{lagniez2017preprocessing}. 
Not all preprocessing methods in model counting are suitable for entropy computation. 
Our research found that the most effective method is literal equivalence, and we have made some improvements to this preprocessing method to make it suitable for entropy computation (in Section \ref{sec:PSE}).

\textbf{Knowledge compilation} %Knowledge compilation compiles propositional theory into a specific target language, which supports querying a variety of queries, including satisfiability, model counting, uniform sampling in polynomial time, etc. 
The motivation for utilizing knowledge compilation for model counting lies in the fact that it is difficult to compute the number of models for the original Boolean formula, whereas the new target language can solve the model counting problem quickly. 
Darwiche et al. first proposed a compiler called c2d~\cite{darwiche2004new} to convert the given CNF formula into Decision-DNNF. 
The core technique of c2d compile CNF to Decision-DNNF is decomposition.
Based on the exact solver sharpsat~\cite{thurley2006sharpsat}, Muise and McIlraith et al. developed a new compiler, Dsharp~\cite{muise2012d}, which converts CNF to Decision-DNNF. 
Unlike c2d, Dsharp utilizes two significant features of sharpSAT: dynamic decomposition and implicit binary constraint propagation. 
Lagniez and Marquis~\cite{lagniez2017improved} proposed an improved Decision-DNNF compiler, called D4, is a top-down compiler that compiles CNF formulas into d-DNNF.
Similar to Dsharp, D4 also uses dynamic decomposition, although the decomposition strategy is different.
In order to better improve the efficiency of the algorithm, D4 makes full use of heuristic strategies such as disjoint component analysis, component caching, conflict analysis and non-technical backtracking, which perform well in c2d and Dsharp. 
Exploiting literal equivalence, Lai et al.~\cite{lai2021power} proposed a generalization of Decision-DNNF, called CCDD, to capture literal equivalence. 
They demonstrate that CCDD supports model counting in linear time and design a model counter called ExactMC based on CCDD.

\begin{comment}
	Later, Darwiche et al. proposed the method of knowledge compilation, which sparked a wave of computing model counting through knowledge compilation, giving birth to many well-known solvers such as c2d~\cite{darwiche2004new}, dsharp~\cite{muise2012d}, d4~\cite{lagniez2017improved}, ExactMC~\cite{lai2021power}, etc.
\end{comment}
%The interaction between logic and probability has garnered significant interest throughout the evolution of AI.
%Within the realm of propositional logic, this interaction has predominantly centered around computational advancements.
%%An important finding is that probabilistic reasoning can be reduced to weighted model counting~\cite{}.

In order to compute the Shannon entropy, the focus of this paper is to design a compiled language that supports the representation of probability distributions.
%Probabilistic inference is a difficult problem in Artificial Intelligence~\cite{dagum1993approximating}.
%Many knowledge compilation languages are used to solve probabilistic problems.
Numerous target representations have been used to concisely model probability distributions, providing more concise factorizations than Bayesian networks in the presence of local structure\cite{dal2021compositional}.
d-DNNF can be used to compile relational Bayesian networks for exact inference~\cite{chavira2006compiling}.
Probabilistic Decision Graph (PDG) is a representation language for probability distributions based on BDD~\cite{jaeger2004probabilistic}.
Probabilistic Sentential Decision Diagram (PSDD) is a complete and canonical representation of probability distributions defined over the models of a given propositional theory~\cite{kisa2014probabilistic}. 
Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD).
And/Or Multi-Valued Decision Diagrams (AOMDDs)~\cite{mateescu2008and} take advantage of the AND/OR structure to represent probability distributions.
Affine ADDs(AADDs)~\cite{sanner2005affine}, an extended form of ADDs, achieve more compression through affine transformations, which is beneficial for probabilistic inference algorithms.
Macii and Poncino~\cite{macii1996exact} demonstrated that ADD enables efficient and precise computation of entropy.
Nevertheless, our experiments revealed that the size of ADD frequently grows exponentially when handling large circuit formulas, suggesting that ADD is not suitable for computing entropy of large-scale circuit formulas.
Based on the core idea of simplifying the size of ADD, we propose an extended form of ADD, ADD-L, which utilizes the power of implied literals to make the graph structure more concise and easier to hit the cache during the construction process.
Apart from ADD, there have been studies utilizing BDDs for approximating entropy.
Stanković et al.~\cite{stankoviccomputing} observed that the process of computing entropy estimates could be implemented on BDDs, thus avoiding redundant calculations and ensuring the efficiency of the process.
They used BDDs to estimate the entropy of a given vector in further studies~\cite{stankovic2007calculating}.
The complexity of their proposed algorithm is proportional to the number of nodes in the decision graph, which is similar to the complexity of our entropy calculation. However, it is worth noting that they only estimated the entropy, while we solve it precisely. 
In addition, the number of ADD-L nodes proposed by us is significantly less than that of BDDs, theoretically making our efficiency much higher.



\begin{comment}
	An efficient approach is trying to find the most concise factorization of the function that computes marginal probabilities, which is also referred to as Knowledge Compilation.
	Numerous target representations have been used to concisely model probability distributions, providing more concise factorizations than Bayesian networks in the presence of local structure\cite{dal2021compositional}.
	Examples include d-DNNF~\cite{chavira2006compiling}, Sentential Decision Diagrams (SDD)~\cite{choi2013compiling}, Probabilistic SDDs (PSDD)~\cite{kisa2014probabilistic}, Reduced Ordered Binary Decision Diagrams (ROBDD)~\cite{nielsen2013using}, Zero-suppressed BDDs (ZBDD)~\cite{minato2007compiling}, And/Or Multi-Valued Decision Diagrams (AOMDDs)~\cite{mateescu2008and}, Probabilistic Decision Graphs (PDG)~\cite{jaeger2004probabilistic}, Weighted Positive BDDs (WPBDD)~\cite{dal2017weighted}, Algebraic Decision Diagrams (ADD)~\cite{dudek2020addmc}, among others.
	
\end{comment}



\begin{comment}
	Many problems in artificial intelligence, after formalization, can be reduced to the problem of calculating the number of models of propositional expressions.
	For example, in the calculation of belief degree and Bayesian belief network in approximate reasoning, approximation can be used to avoid computational difficulties and reduce them to the problem of model counting in the propositional domain~\cite{roth1996hardness}. 
	The probabilistic inference problem can be simplified to the weighted model counting problem on the propositional knowledge base by encoding the Bayesian network into a CNF knowledge base and assigning weights to the CNF variables according to the network probability~\cite{chavira2008probabilistic}. 
	The purpose of assigning weights to variables is to enable each CNF model to generate a weight, which allows people to express the probability of certain evidence as the sum of weights of models consistent with the evidence.
	
	
	In the model counting problem, the algorithm can be divided into exact model counting and approximate model counting based on the accuracy of the solution.
	Exact model counting can accurately solve all the model numbers of the propositional formula, while approximate model counting can only find the model numbers with certain errors or the upper and lower bounds of the real model numbers. 
	Accurate solutions are expected, but at the same time, they take a lot of time to solve, and can only deal with small scale problems, while the approximate model counting sacrifices a certain precision, gets stronger scalability and robustness, and can solve larger scale problems.
	
	
	Therefore, KC-based reasoning techniques are widely used in probabilistic databases~\cite{van2017query}, probabilistic programming~\cite{fierens2015inference}, processable learning~\cite{kisa2014probabilistic}, and synthesis and verification of hardware and software systems~\cite{fried2016bdd}.
	
	Therefore, the compiled formula needs to meet two criteria. Firstly, the size of the formula must be polynomial, otherwise the conversion would be meaningless. Secondly, the speed of computing the model count for the formula must be fast, requiring at least polynomial time complexity, or even linear time complexity.
	In the last two decades, deterministic Decomposable Negation Normal Form (d-DNNF) is a language of wide interest to researchers, supporting the computation of models in polynomial time (within compilation size). 
	In practical applications, binary decision serves as a crucial property that enforces determinism in compiler design~\cite{lagniez2017improved}, and the resulting subset of d-DNNF is referred to as Decision-DNNF~\cite{oztok2014compiling}.
	The state-of-the-art model counter based on knowledge compilation focuses on compilation to a  Decision-DNNF for model counting.
	
	Model counting ($\#$SAT) is the problem of computing the number
	of satisfying assignments for a given propositional formula. It is a fundamental
	problem with a wide variety of applications ranging from
	probabilistic inference~\cite{roth1996hardness, chavira2008probabilistic} , neural network verification~\cite{baluta2019quantitative} , network reliability~\cite{duenas2017counting}, computational biology, and the like. Valiant proved that the problem of model counting is $\#P$ complete~\cite{valiant1979complexity}, where $\#P$ is a set of enumeration problems related to NP decision problems. Later, Todd's pioneering work proved that $PH \subseteq P^{\# P}$  ~\cite{toda1989computational}.
\end{comment}




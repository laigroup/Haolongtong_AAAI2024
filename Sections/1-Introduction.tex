\section{Introduction}
\label{sec:Intro}

Quantitative information flow (QIF) is an important approach to measuring the amount of information leaked about a secret by observing the running of a program~\cite{denning1982cryptography, gray1992toward}.
In QIF, we often quantify the leakage using entropy-theoretic notions, such as Shannon entropy~\cite{backes2009automatic, cerny2011complexity, phan2012symbolic, smith2009foundations} or
min-entropy~\cite{backes2009automatic, meng2011calculating, phan2012symbolic, smith2009foundations}.
Roughly speaking, a program in QIF can be seen as a function from a set of secret inputs $X$ to outputs $Y$ observable to an attacker who may try to infer $X$ based on the output $Y$.
Boolean clausal constraints are a basic representation to model programs~\cite{fremont2017maximum, golia2022scalable}. 
In this paper, we focus on precisely computing the Shannon entropy of a program expressed in Boolean clausal constraints.

Let $\varphi(X,Y)$ be the Boolean formula modeling the relationship between the input variable set $X$ and the output variable set $Y$ in the given program.
We require that for any assignment of $X$, there is at most one assignment of $Y$ that satisfies the formula  $\varphi(X,Y)$.
Let $p$ represent a probability distribution defined over the set $\{0,1\}^Y$.
For each assignment $\sigma$ to $Y$, i.e., $\sigma:Y \mapsto \sigma$, the probability is defined as $p_{\sigma} = \frac{\left| \mathit{Sol}(\varphi(Y \mapsto \sigma)) \right|}{ \left| \mathit{Sol}(\varphi)_{\downarrow X} \right| }$, where $\mathit{Sol}(\varphi(Y \mapsto \sigma))$ denotes the set of solutions of $\varphi(Y \mapsto \sigma)$ and $\mathit{Sol}(\varphi)_{\downarrow X}$ denotes the set of solutions of $\varphi$ projected to $X$.
The Shannon entropy of $\varphi$ is $H(\varphi) = \sum_{\sigma \in 2^Y} -p_{\sigma} \log p_{\sigma} $.
Then we can immediately obtain a measure of leaked information with the computed entropy and the assumption that $X$ follows a uniform distribution~\footnote{If $X$ does not follow a uniform distribution, techniques exist
for reducing the analysis to a uniform case~\cite{backes2011non}.}~\cite{klebanov2013sat}. 

The workflow of the current precise methods of computing entropy can often be divided into two stages. 
In the first stage, we enumerate possible outputs, i.e., the satisfying assignments over $Y$, while in the second stage, we compute the probability of the current output based on the number of inputs mapped to the output~\cite{golia2022scalable}.
The computation in the second stage often invokes model counting (\#SAT), which refers to computing the number of solutions $\mathit{Sol}(\varphi)$ for a given set of clausal constraints $\varphi$. 
Due to the exponential possible outputs, the current precise methods are often difficult to scale to programs with a large size of $Y$.
While this paper focuses on improving the precise computation of entropy, we remark that Priyanka et al.~\cite{golia2022scalable} proposed the first Shannon entropy estimation tool, EntropyEstimation, that guarantees that the estimate lies within $(1 \pm \epsilon)$-factor of $H(\varphi)$ with confidence at least $1-\delta$.
EntropyEstimation uses uniform sampling to avoid generating all outputs, and indeed scales much better than the precise methods. 

As we mentioned previously, the current methods for precise Shannon entropy computing are difficult to scale to clausal constraints with a large set of outputs.
Theoretically, we sometimes need to perform $2^{|Y|}$ model counting queries. 
The main contribution of this paper is to improve the scalability of precise computation of Shannon entropy. 
We enhance the computation process in both stages of precise Shannon entropy computing.
For the first stage, we design a knowledge compilation language to guide the search to avoid exhausting the possible outputs. 
The language combines Algebraic Decision Diagrams (ADD), an influential representation, and implied literals, an important notion in Boolean satisfiability solving.
For the second stage, we do not individually perform model counting queries.
Instead, we share the component caching with the successive model counting queries.
Moreover, we take advantage of literal equivalence to pre-process the clausal constraints corresponding to a given program.
Integrating all the techniques mentioned above, we propose a Precise Shannon Entropy tool PSE.
We conducted an extensive experimental evaluation over a comprehensive set of benchmarks (361 in total) and compared PSE with the existing precise Shannon entropy computing methods and the current state-of-the-art Shannon entropy estimation tool, EntropyEstimation.
Our experiments show that the existing precise Shannon entropy algorithm solves only 17 instances, while PSE can solve 289 instances, representing a significant improvement of 272 instances.
EntropyEstimation can solve 264 instances, while PSE can solve 25 more instances, a surprising improvement.

The rest of the paper is structured as follows. 
We present notation and background in Section \ref{sec:Notation}.
We introduce Algebraic Decision Diagrams (ADD) with implication literals in Section \ref{sec:ADDL}.
Section \ref{sec:PSE} presents the application of ADD-L to QIF and introduces our precise entropy tool, PSE. 
Section \ref{sec:Experiments} details the specific results and analysis of the experiments.
Section \ref{sec:Related} discusses related work.
Finally, we conclude in Section \ref{sec:Conclusion}.

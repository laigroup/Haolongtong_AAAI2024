\section{Introduction}
\label{sec:Intro}

Quantitative information flow (QIF) is an important approach to measuring the amount of information leaked about a secret by observing the running of a program~\cite{denning1982cryptography, gray1992toward}.
In QIF, we often quantify the leakage using entropy-theoretic notions, such as Shannon entropy~\cite{backes2009automatic, cerny2011complexity, phan2012symbolic, smith2009foundations} or
min-entropy~\cite{backes2009automatic, meng2011calculating, phan2012symbolic, smith2009foundations}.
Roughly speaking, a program in QIF can be seen as a function from a set of secret inputs $X$ to outputs $Y$ observable to an attacker who may try to infer $X$ based on the output $Y$.
Boolean clausal constraints are a basic representation to model programs~\cite{fremont2017maximum, golia2022scalable}. 
In this paper, we focus on precisely computing the Shannon entropy of a program expressed in Boolean clausal constraints.


Let $\varphi(X,Y)$ be a Boolean formula that models the relationship between the input variable set $X$ and the output variable set $Y$ in a given program, such that for any assignment of $X$, at most one assignment of $Y$ satisfies the formula $\varphi(X,Y)$".
Let $p$ represent a probability distribution defined over the set $\{0,1\}^Y$.
For each assignment $\sigma$ to $Y$, i.e., $\sigma:Y \mapsto \sigma$, the probability is defined as $p_{\sigma} = \frac{\left| \mathit{Sol}(\varphi(Y \mapsto \sigma)) \right|}{ \left| \mathit{Sol}(\varphi)_{\downarrow X} \right| }$, where $\mathit{Sol}(\varphi(Y \mapsto \sigma))$ denotes the set of solutions of $\varphi(Y \mapsto \sigma)$ and $\mathit{Sol}(\varphi)_{\downarrow X}$ denotes the set of solutions of $\varphi$ projected to $X$.
The Shannon entropy of $\varphi$ is $H(\varphi) = \sum_{\sigma \in 2^Y} -p_{\sigma} \log p_{\sigma} $.
Then we can immediately obtain a measure of leaked information with the computed entropy and the assumption that $X$ follows a uniform distribution~\footnote{If $X$ does not follow a uniform distribution, techniques exist
	for reducing the analysis to a uniform case~\cite{backes2011non}.}~\cite{klebanov2013sat}. 

The workflow of the current precise methods of computing \include{main}entropy can often be divided into two stages. 
In the first stage, we enumerate possible outputs, i.e., the satisfying assignments over $Y$. 
In the second stage, we calculate the probability of each output based on the count of inputs that map to that output~\cite{golia2022scalable}.
The computation in the second stage often invokes model counting (\#SAT), which refers to computing the number of solutions $\mathit{Sol}(\varphi)$ for a given set of clausal constraints $\varphi$. 
Due to the exponential possible outputs, current precise methods often struggle to scale for programs with a large size of $Y$.
Therefore, more researchers are focusing on approximate estimation of Shannon entropy.
We remark that Golia et al.~\cite{golia2022scalable} proposed the first Shannon entropy estimation tool, EntropyEstimation, which guarantees that the estimate lies within $(1 \pm \epsilon)$-factor of $H(\varphi)$ with confidence at least $1-\delta$.
EntropyEstimation employs uniform sampling to avoid generating all outputs, and indeed scales much better than the precise methods. 

As previously discussed, current methods for precisely computing Shannon entropy struggle to scale with clausal constraints involving large sets of outputs.
Theoretically, this requires performing up to $2^{|Y|}$ model counting queries.
The primary contribution of this paper is to enhance the scalability of precise Shannon entropy computation by improving both stages of the computation process.
For the first stage, we design a knowledge compilation language to guide the search to avoid exhausting the possible outputs. 
This language integrates Algebraic Decision Diagrams (ADD), an influential representation, and implied literals, an important notion in Boolean satisfiability solving.
For the second stage, instead of performing model counting queries individually, we leverage shared component caching across successive queries.
Moreover, we take advantage of literal equivalence to pre-process the clausal constraints corresponding to a given program.
Integrating all the techniques mentioned above, we propose a Precise Shannon Entropy tool PSE.
We conducted an extensive experimental evaluation over a comprehensive set of benchmarks (361 in total) and compared PSE with the existing precise Shannon entropy computing methods and the current state-of-the-art Shannon entropy estimation tool, EntropyEstimation. % to do
Our experiments indicate that the existing precise Shannon entropy algorithm is capable of solving only 17 instances, whereas PSE can address 289 instances, marking a significant improvement of 272 instances.
EntropyEstimation is able to solve 264 instances, whereas PSE surpasses this by solving an additional 25 instances, which is a surprising improvement.

The rest of the paper is structured as follows. 
We present notation and background in Section \ref{sec:Notation}.
We introduce Algebraic Decision Diagrams (ADD) with implication literals in Section \ref{sec:ADDL}.
Section \ref{sec:PSE} presents the application of ADD-L to QIF and introduces our precise entropy tool, PSE. 
Section \ref{sec:Experiments} details the specific results and analysis of the experiments.
Section \ref{sec:Related} discusses related work.
Finally, we conclude in Section \ref{sec:Conclusion}.
